{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139969, 0.48215842, 0.44653093], [0.20220212, 0.19931542, 0.20086347])\n",
    "])\n",
    "training_data = CIFAR10('cifar/train', train=True, download=True, transform=transform)\n",
    "test_data = CIFAR10('cifar/test', train=False, download=True, transform=transform)\n",
    "loader = DataLoader(training_data, 32, True)\n",
    "loader_test = DataLoader(test_data, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-322-09c843c47ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstds\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcalc_normalize_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-322-09c843c47ac0>\u001b[0m in \u001b[0;36mcalc_normalize_values\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmeans\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mstds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tobytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculate dataset mean and std per channel to feed to transform normalization params\n",
    "def calc_normalize_values():\n",
    "    means = np.zeros(3)\n",
    "    stds = np.zeros(3)\n",
    "    N = len(training_data)\n",
    "    for i, (data, label) in enumerate(training_data):\n",
    "        means += data.numpy().mean(axis=(1,2))\n",
    "        stds += data.numpy().std(axis=(1,2))\n",
    "    means /= N\n",
    "    stds /= N\n",
    "    print(means, stds)\n",
    "calc_normalize_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "(data, labels) = next(iter(loader))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3573adf128>"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = data[5].numpy()\n",
    "plt.imshow(im.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[5].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "p={}\n",
    "p['W1'] = torch.randn(32, 3, 5, 5, dtype=torch.float32) * np.sqrt(2. / 3 * 5 * 5)\n",
    "p['W1'].requires_grad = True\n",
    "p['b1'] = torch.zeros(32, dtype=torch.float32, requires_grad=True)\n",
    "p['W2'] = torch.randn(16, 32, 5, 5, dtype=torch.float32) * np.sqrt(2. / 32 * 5 * 5)\n",
    "p['W2'].requires_grad = True\n",
    "p['b2'] = torch.zeros(16, dtype=torch.float32, requires_grad=True)\n",
    "p['W3'] = torch.randn(16*32*32, 10, dtype=torch.float32) * np.sqrt(2. / 16 * 32 * 32)\n",
    "p['W3'].requires_grad = True\n",
    "p['b3'] = torch.zeros(10, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet(x, params):\n",
    "    a1 = F.conv2d(x, weight=params['W1'], bias=params['b1'], padding=2)\n",
    "    z1 = a1.clamp(0)\n",
    "    a2 = F.conv2d(z1, weight=params['W2'], bias=params['b2'], padding=2)\n",
    "    z2 = a2.clamp(0)\n",
    "    z2 = z2.view(z2.shape[0], -1)\n",
    "    a3 = z2.mm(params['W3']) + params['b3']\n",
    "    return a3\n",
    "\n",
    "#scores = convnet(data, p)\n",
    "#loss = torch.nn.CrossEntropyLoss()\n",
    "#cost = loss(scores, labels)\n",
    "#cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312.5"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader_test.dataset)/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy():\n",
    "    acc = 0\n",
    "    num_samples = 0\n",
    "    for i, (data, labels) in enumerate(loader_test):\n",
    "        scores = convnet(data, p)\n",
    "        res = scores.argmax(dim=1)\n",
    "        correct = res == labels\n",
    "        acc += correct.sum().float()/labels.shape[0]\n",
    "        num_samples += 1\n",
    "    acc /= num_samples\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, cost is 1124151.375000, test accuracy is 0.114217\n",
      "iteration 100, cost is nan, test accuracy is 0.100040\n",
      "iteration 200, cost is nan, test accuracy is 0.100040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-330-dee343405458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, (data, labels) in enumerate(loader):\n",
    "    scores = convnet(data, p)\n",
    "    cost = F.cross_entropy(scores, labels)\n",
    "    cost.backward()\n",
    "    with torch.no_grad():\n",
    "        for k,w in p.items():\n",
    "            w -= 3e-3 * w.grad\n",
    "            w.grad.zero_()\n",
    "    if idx % 100 == 0:\n",
    "        print('iteration %d, cost is %f, test accuracy is %f' % (idx, float(cost), check_accuracy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'float' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-285-784c2c7c4168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;36m2.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'float' and 'tuple'"
     ]
    }
   ],
   "source": [
    "a=(2,3,4)\n",
    "2./a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
