{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139969, 0.48215842, 0.44653093], [0.20220212, 0.19931542, 0.20086347])\n",
    "])\n",
    "training_data = CIFAR10('cifar/train', train=True, download=True, transform=transform)\n",
    "test_data = CIFAR10('cifar/test', train=False, download=True, transform=transform)\n",
    "loader = DataLoader(training_data, 32, sampler=sampler.SubsetRandomSampler(range(49000)))\n",
    "loader_val = DataLoader(training_data, 32, sampler=sampler.SubsetRandomSampler(range(49000,50000)))\n",
    "loader_test = DataLoader(test_data, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-09c843c47ac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstds\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcalc_normalize_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-09c843c47ac0>\u001b[0m in \u001b[0;36mcalc_normalize_values\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mmeans\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mstds\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\datasets\\cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \"\"\"\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculate dataset mean and std per channel to feed to transform normalization params\n",
    "def calc_normalize_values():\n",
    "    means = np.zeros(3)\n",
    "    stds = np.zeros(3)\n",
    "    N = len(training_data)\n",
    "    for i, (data, label) in enumerate(training_data):\n",
    "        means += data.numpy().mean(axis=(1,2))\n",
    "        stds += data.numpy().std(axis=(1,2))\n",
    "    means /= N\n",
    "    stds /= N\n",
    "    print(means, stds)\n",
    "calc_normalize_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "(data, labels) = next(iter(loader))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a9e07e6588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = data[5].numpy()\n",
    "plt.imshow(im.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('using ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p={}\n",
    "W1 = torch.randn((32, 3, 5, 5), dtype=torch.float32, device=device) * np.sqrt(2. / (3*5*5))\n",
    "W1.requires_grad = True\n",
    "b1 = torch.zeros((32,), dtype=torch.float32, requires_grad=True, device=device)\n",
    "W2 = torch.randn((16, 32, 3, 3), dtype=torch.float32, device=device) * np.sqrt(2. / (32*3*3))\n",
    "W2.requires_grad = True\n",
    "b2 = torch.zeros((16,), dtype=torch.float32, requires_grad=True, device=device)\n",
    "W3 = torch.randn((16*32*32, 10), dtype=torch.float32, device=device) * np.sqrt(2. / (16*32*32))\n",
    "W3.requires_grad = True\n",
    "b3 = torch.zeros((10,), dtype=torch.float32, requires_grad=True, device=device)\n",
    "p=(W1, b1, W2, b2, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet(x, params):\n",
    "    (W1, b1, W2, b2, W3, b3) = params\n",
    "    a1 = F.conv2d(x, weight=W1, bias=b1, padding=2)\n",
    "    z1 = F.relu(a1)\n",
    "    a2 = F.conv2d(z1, weight=W2, bias=b2, padding=1)\n",
    "    z2 = F.relu(a2)\n",
    "    z2 = z2.view(z2.shape[0], -1)\n",
    "    a3 = z2.mm(W3) + b3\n",
    "    return a3\n",
    "\n",
    "#scores = convnet(data, p)\n",
    "#loss = torch.nn.CrossEntropyLoss()\n",
    "#cost = loss(scores, labels)\n",
    "#cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader_test.dataset)/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy():\n",
    "    acc = 0\n",
    "    num_samples = 0\n",
    "    for i, (data, labels) in enumerate(loader_val):\n",
    "        scores = convnet(data, p)\n",
    "        res = scores.argmax(dim=1)\n",
    "        correct = res == labels\n",
    "        acc += correct.sum().float()/labels.shape[0]\n",
    "        num_samples += 1\n",
    "    acc /= num_samples\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, cost is 8041.079102, test accuracy is 0.106445\n",
      "iteration 100, cost is 6.596337, test accuracy is 0.091797\n",
      "iteration 200, cost is 3.649671, test accuracy is 0.103516\n",
      "iteration 300, cost is 3.749621, test accuracy is 0.075195\n",
      "iteration 400, cost is 2.383446, test accuracy is 0.074219\n",
      "iteration 500, cost is 2.248861, test accuracy is 0.106445\n",
      "iteration 600, cost is 2.301419, test accuracy is 0.106445\n"
     ]
    }
   ],
   "source": [
    "for idx, (data, labels) in enumerate(loader):\n",
    "    data = data.to(device=device, dtype=torch.float32)\n",
    "    labels = labels.to(device=device, dtype=torch.long)\n",
    "    scores = convnet(data, p)\n",
    "    cost = F.cross_entropy(scores, labels)\n",
    "    cost.backward()\n",
    "    with torch.no_grad():\n",
    "        for w in p:\n",
    "            w -= 3e-3 * w.grad\n",
    "            w.grad.zero_()\n",
    "    if idx % 100 == 0:\n",
    "        print('iteration %d, cost is %f, test accuracy is %f' % (idx, float(cost), check_accuracy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'float' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-285-784c2c7c4168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;36m2.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'float' and 'tuple'"
     ]
    }
   ],
   "source": [
    "a=(2,3,4)\n",
    "2./a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
