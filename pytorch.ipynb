{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139969, 0.48215842, 0.44653093], [0.20220212, 0.19931542, 0.20086347])\n",
    "])\n",
    "training_data = CIFAR10('cifar/train', train=True, download=True, transform=transform)\n",
    "test_data = CIFAR10('cifar/test', train=False, download=True, transform=transform)\n",
    "loader = DataLoader(training_data, 32, sampler=sampler.SubsetRandomSampler(range(49000)))\n",
    "loader_val = DataLoader(training_data, 32, sampler=sampler.SubsetRandomSampler(range(49000,50000)))\n",
    "loader_test = DataLoader(test_data, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-09c843c47ac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstds\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcalc_normalize_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-09c843c47ac0>\u001b[0m in \u001b[0;36mcalc_normalize_values\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mmeans\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mstds\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\datasets\\cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \"\"\"\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;31m# TODO: make efficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculate dataset mean and std per channel to feed to transform normalization params\n",
    "def calc_normalize_values():\n",
    "    means = np.zeros(3)\n",
    "    stds = np.zeros(3)\n",
    "    N = len(training_data)\n",
    "    for i, (data, label) in enumerate(training_data):\n",
    "        means += data.numpy().mean(axis=(1,2))\n",
    "        stds += data.numpy().std(axis=(1,2))\n",
    "    means /= N\n",
    "    stds /= N\n",
    "    print(means, stds)\n",
    "calc_normalize_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "(data, labels) = next(iter(loader))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0054c8e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = data[5].numpy()\n",
    "plt.imshow(im.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('using ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p={}\n",
    "W1 = torch.randn((32, 3, 5, 5), dtype=torch.float32, device=device) * 0.01\n",
    "W1.requires_grad = True\n",
    "b1 = torch.zeros((32,), dtype=torch.float32, requires_grad=True, device=device)\n",
    "W2 = torch.randn((16, 32, 3, 3), dtype=torch.float32, device=device) * 0.01\n",
    "W2.requires_grad = True\n",
    "b2 = torch.zeros((16,), dtype=torch.float32, requires_grad=True, device=device)\n",
    "W3 = torch.randn((16*32*32, 10), dtype=torch.float32, device=device) * 0.01\n",
    "W3.requires_grad = True\n",
    "b3 = torch.zeros((10,), dtype=torch.float32, requires_grad=True, device=device)\n",
    "p=(W1, b1, W2, b2, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet(x, params):\n",
    "    (W1, b1, W2, b2, W3, b3) = params\n",
    "    a1 = F.conv2d(x, weight=W1, bias=b1, padding=2)\n",
    "    z1 = F.relu(a1)\n",
    "    a2 = F.conv2d(z1, weight=W2, bias=b2, padding=1)\n",
    "    z2 = F.relu(a2)\n",
    "    z2 = z2.view(z2.shape[0], -1)\n",
    "    a3 = z2.mm(W3) + b3\n",
    "    return a3\n",
    "\n",
    "#scores = convnet(data, p)\n",
    "#loss = torch.nn.CrossEntropyLoss()\n",
    "#cost = loss(scores, labels)\n",
    "#cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader_test.dataset)/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy():\n",
    "    acc = 0\n",
    "    num_samples = 0\n",
    "    for i, (data, labels) in enumerate(loader_val):\n",
    "        scores = convnet(data, p)\n",
    "        res = scores.argmax(dim=1)\n",
    "        correct = res == labels\n",
    "        acc += correct.sum().float()/labels.shape[0]\n",
    "        num_samples += 1\n",
    "    acc /= num_samples\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, cost is 2.306518, test accuracy is 0.076172\n",
      "iteration 100, cost is 2.301240, test accuracy is 0.180664\n",
      "iteration 200, cost is 2.220878, test accuracy is 0.195312\n",
      "iteration 300, cost is 2.041968, test accuracy is 0.258789\n",
      "iteration 400, cost is 2.037443, test accuracy is 0.301758\n",
      "iteration 500, cost is 2.074458, test accuracy is 0.334961\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ff8896eea352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stig\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, (data, labels) in enumerate(loader):\n",
    "    data = data.to(device=device, dtype=torch.float32)\n",
    "    labels = labels.to(device=device, dtype=torch.long)\n",
    "    scores = convnet(data, p)\n",
    "    cost = F.cross_entropy(scores, labels)\n",
    "    cost.backward()\n",
    "    with torch.no_grad():\n",
    "        for w in p:\n",
    "            w -= 3e-3 * w.grad\n",
    "            w.grad.zero_()\n",
    "    if idx % 100 == 0:\n",
    "        print('iteration %d, cost is %f, test accuracy is %f' % (idx, float(cost), check_accuracy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'float' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-285-784c2c7c4168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;36m2.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'float' and 'tuple'"
     ]
    }
   ],
   "source": [
    "a=(2,3,4)\n",
    "2./a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
