{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading the mnist dataset from the keras datasets api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x149898ba6d8>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[4], cmap = matplotlib.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the (28,28) dimensional matrix to a vector of size (28*28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = x / 255.\n",
    "    x = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))\n",
    "    return x \n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    res = np.zeros((y.shape[0], num_classes))\n",
    "    res[np.arange(y.shape[0]), y] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = preprocess(train_x)\n",
    "train_x = train_x.T\n",
    "train_y = to_categorical(train_y, 10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x149899be828>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[:, 5200].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:,5200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is an activation function that outputs probabilities for multi-class classification problems. It is similar to a sigmoid output which is often used for outputing a single probability. Softmax makes sure that the sum of the individual probabilities equals to 1. \n",
    "\n",
    "Softmax formula:\n",
    "\n",
    "$p_i = \\dfrac{e^{a_i}}{\\sum^N_{k=1} e^{a_k}}$ - Equation 1: Softmax\n",
    "\n",
    "By using the exponential function it gives more weight to higher probabilities. It gives an advantage to higher values. Therefore it's called a soft max function. A max function would give 100% probability to the highest value, softmax is somewhere in between max and an actual probability as given by equation 2:\n",
    "\n",
    "$p_i = \\dfrac{a_i}{\\sum^N_{k=1} a_k}$ - Equation 2: Standard linear probability\n",
    "\n",
    "There is however a problem with using the regular softmax function, as it uses an exponential function chances are high that it will encounter an overflow. To overcome this, we can subtract the a values by its maximum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_term = np.exp(a) #-max to Prevent overflow\n",
    "    res = exp_term/np.sum(exp_term, axis=0)\n",
    "    return res\n",
    "\n",
    "def cross_entropy_loss(outputs, y):\n",
    "    loss = np.sum(y * np.log(outputs))\n",
    "    return loss * -1./outputs.shape[1] #average loss of all samples\n",
    "    \n",
    "def relu(a):\n",
    "    return np.maximum(0, a)\n",
    "\n",
    "def tanh(a):\n",
    "    return np.tanh(a)\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(a, W, B, activation):\n",
    "    z = W.dot(a) + B\n",
    "    next_a = activation(z)\n",
    "    return next_a, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 60000\n",
      "0.09953333333333333\n",
      "10.583920471035105\n",
      "0.10151666666666667\n",
      "8.115722961649944\n",
      "0.17141666666666666\n",
      "6.117453087383453\n",
      "0.15905\n",
      "5.455746101680998\n",
      "0.2608666666666667\n",
      "4.47439517465885\n",
      "0.23476666666666668\n",
      "4.170953528167493\n",
      "0.3292833333333333\n",
      "3.6584370997331153\n",
      "0.3308\n",
      "3.4719402154601227\n",
      "0.3879666666666667\n",
      "3.098490604987994\n",
      "0.3970666666666667\n",
      "2.8851346197981513\n",
      "0.43233333333333335\n",
      "2.7483098257796055\n",
      "0.45175\n",
      "2.5856600316522824\n",
      "0.47401666666666664\n",
      "2.4217128333669087\n",
      "0.49448333333333333\n",
      "2.2943638344327253\n",
      "0.5112333333333333\n",
      "2.1699460278408753\n",
      "0.5280666666666667\n",
      "2.079096171423668\n",
      "0.54345\n",
      "1.9680090921102251\n",
      "0.556\n",
      "1.9045865462164113\n",
      "0.5711666666666667\n",
      "1.8107996661552161\n",
      "0.5795666666666667\n",
      "1.7630652721317555\n",
      "0.5940666666666666\n",
      "1.6857205353550986\n",
      "0.59985\n",
      "1.6451578874868638\n",
      "0.6138833333333333\n",
      "1.5810532977009562\n",
      "0.6188166666666667\n",
      "1.5444237813594377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-208-e3acbd6faf01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-208-e3acbd6faf01>\u001b[0m in \u001b[0;36mbackward_pass\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mda1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mdz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mda1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mdw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdz1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mdb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class model():\n",
    "    \n",
    "    def __init__(self, input_data):\n",
    "        self.learning_rate = 1\n",
    "        self.x = input_data\n",
    "        self.n, self.m = input_data.shape\n",
    "        print(self.n, self.m)\n",
    "        \n",
    "        self.first_layer_nodes = 128\n",
    "        self.output_layer_nodes = 10\n",
    "\n",
    "        self.W1 = np.random.randn(self.first_layer_nodes, self.n)\n",
    "        self.B1 = np.random.random((self.first_layer_nodes, 1))\n",
    "        self.W2 = np.random.randn(self.output_layer_nodes, self.first_layer_nodes)\n",
    "        self.B2 = np.random.random((self.output_layer_nodes, 1))\n",
    "    \n",
    "    def forward_pass(self): \n",
    "        self.z1 = self.W1.dot(self.x) + self.B1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = self.W2.dot(self.a1) + self.B2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        return cross_entropy_loss(self.a2, train_y)\n",
    "    \n",
    "    def backward_pass(self, epoch):\n",
    "        dz2 = (self.a2 - train_y)\n",
    "        dw2 = dz2.dot(self.a1.T) * 1./self.m\n",
    "        db2 = np.sum(dz2, axis=1, keepdims=True)* 1./self.m\n",
    "        \n",
    "        da1 = self.W2.T.dot(dz2)\n",
    "        dz1 = da1 * sigmoid(self.z1) * (1 - sigmoid(self.z1))\n",
    "        dw1 = dz1.dot(self.x.T)* 1./self.m\n",
    "        db1 = np.sum(dz1, axis=1, keepdims=True)* 1./self.m\n",
    "        \n",
    "        self.W2 = self.W2 - self.learning_rate * dw2\n",
    "        self.B2 = self.B2 - self.learning_rate * db2\n",
    "        self.W1 = self.W1 - self.learning_rate * dw1\n",
    "        self.B1 = self.B1 - self.learning_rate * db1\n",
    "        \n",
    "        \n",
    "    def error():\n",
    "        pass\n",
    "\n",
    "m = model(train_x)\n",
    "for i in range(100):\n",
    "    loss = m.forward_pass()\n",
    "    predictions = np.argmax(m.a2, axis=0)\n",
    "    correct = np.argmax(train_y, axis=0)\n",
    "    print(np.sum(np.array(predictions == correct))/m.a2.shape[1])\n",
    "    print(loss)\n",
    "\n",
    "    m.backward_pass(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = Sequential()\n",
    "km.add(Dense(128, activation='tanh', input_shape=(28*28,)))\n",
    "km.add(Dense(10, activation='softmax'))\n",
    "km.compile(optimizer=\"SGD\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.4927 - acc: 0.0895\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.4589 - acc: 0.0936\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 2.4282 - acc: 0.1002\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.4000 - acc: 0.1077\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.3739 - acc: 0.1165\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.3496 - acc: 0.1258\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.3267 - acc: 0.1355\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.3050 - acc: 0.1463\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.2844 - acc: 0.1580\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.2646 - acc: 0.1701\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.2455 - acc: 0.1833\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.2271 - acc: 0.1974\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 2.2093 - acc: 0.2119\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1919 - acc: 0.2261\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.1749 - acc: 0.2417\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1584 - acc: 0.2557\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1422 - acc: 0.2695\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1263 - acc: 0.2831\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1107 - acc: 0.2959\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0953 - acc: 0.3074\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.0803 - acc: 0.3189\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0654 - acc: 0.3308\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0508 - acc: 0.3421\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0364 - acc: 0.3529\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0222 - acc: 0.3641\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0081 - acc: 0.3743\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9943 - acc: 0.3851\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9807 - acc: 0.3954\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9672 - acc: 0.4052\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9539 - acc: 0.4157\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9407 - acc: 0.4258\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9278 - acc: 0.4363\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9149 - acc: 0.4455\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9023 - acc: 0.4549\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8898 - acc: 0.4640\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8774 - acc: 0.4730\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8652 - acc: 0.4814\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8531 - acc: 0.4893\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8412 - acc: 0.4974\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8294 - acc: 0.5051\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.8178 - acc: 0.5129\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.8063 - acc: 0.5209\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.7949 - acc: 0.5279\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7837 - acc: 0.5353\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.7725 - acc: 0.5425\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7616 - acc: 0.5493\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7507 - acc: 0.5558\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7400 - acc: 0.5627\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7294 - acc: 0.5689\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7189 - acc: 0.5748\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7086 - acc: 0.5802\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.6984 - acc: 0.5859\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6883 - acc: 0.5916\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.6783 - acc: 0.5966\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6684 - acc: 0.6014\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6586 - acc: 0.6063\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6490 - acc: 0.6112\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6395 - acc: 0.6152\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6300 - acc: 0.6193\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6207 - acc: 0.6237\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6115 - acc: 0.6278\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6024 - acc: 0.6317\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5934 - acc: 0.6360\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5845 - acc: 0.6395\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5757 - acc: 0.6436\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5671 - acc: 0.6476\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5585 - acc: 0.6514\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5500 - acc: 0.6546\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5416 - acc: 0.6577\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5333 - acc: 0.6609\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5251 - acc: 0.6642\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5170 - acc: 0.6672\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5090 - acc: 0.6703\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5010 - acc: 0.6733\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4932 - acc: 0.6759\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4855 - acc: 0.6789\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4778 - acc: 0.6817\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4702 - acc: 0.6843\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4628 - acc: 0.6869\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4554 - acc: 0.6889\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4480 - acc: 0.6914\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4408 - acc: 0.6933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4336 - acc: 0.6952\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4266 - acc: 0.6976\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4196 - acc: 0.6996\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4126 - acc: 0.7017\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4058 - acc: 0.7037\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3990 - acc: 0.7056\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3923 - acc: 0.7075\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3857 - acc: 0.7096\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3792 - acc: 0.7116\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3727 - acc: 0.7134\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3663 - acc: 0.7153\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3600 - acc: 0.7169\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3537 - acc: 0.7186\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3475 - acc: 0.7207\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3414 - acc: 0.7221\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3353 - acc: 0.7238\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3293 - acc: 0.7254\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3234 - acc: 0.7270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e51703d9e8>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(train_x, train_y, batch_size=60000, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.array([[1,2,3],[1,4,2]])\n",
    "np.argmax(a, axis=1)\n",
    "a.max(axis=1, keepdims=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/2015xli/multilayer-perceptron/blob/master/multilayer-perceptron-batch.ipynb\n",
    "https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/\n",
    "\n",
    "Why initialize with randn?  Really good insights!!\n",
    "https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
