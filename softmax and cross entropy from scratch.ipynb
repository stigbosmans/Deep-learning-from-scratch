{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading the mnist dataset from the keras datasets api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x149897d8780>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[4], cmap = matplotlib.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the (28,28) dimensional matrix to a vector of size (28*28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))\n",
    "    x = x / 255.\n",
    "    return x \n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    res = np.zeros((y.shape[0], num_classes))\n",
    "    res[np.arange(y.shape[0]), y] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = preprocess(train_x)\n",
    "train_x = train_x.T\n",
    "train_y = to_categorical(train_y, 10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is an activation function that outputs probabilities for multi-class classification problems. It is similar to a sigmoid output which is often used for outputing a single probability. Softmax makes sure that the sum of the individual probabilities equals to 1. \n",
    "\n",
    "Softmax formula:\n",
    "\n",
    "$p_i = \\dfrac{e^{a_i}}{\\sum^N_{k=1} e^{a_k}}$ - Equation 1: Softmax\n",
    "\n",
    "By using the exponential function it gives more weight to higher probabilities. It gives an advantage to higher values. Therefore it's called a soft max function. A max function would give 100% probability to the highest value, softmax is somewhere in between max and an actual probability as given by equation 2:\n",
    "\n",
    "$p_i = \\dfrac{a_i}{\\sum^N_{k=1} a_k}$ - Equation 2: Standard linear probability\n",
    "\n",
    "There is however a problem with using the regular softmax function, as it uses an exponential function chances are high that it will encounter an overflow. To overcome this, we can subtract the a values by its maximum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_term = np.exp(a) #-max to Prevent overflow\n",
    "    res = exp_term/np.sum(exp_term, axis=0)\n",
    "    return res\n",
    "\n",
    "def cross_entropy_loss(outputs, y):\n",
    "    loss = np.sum(y * np.log(outputs))\n",
    "    return loss * -1./outputs.shape[1] #average loss of all samples\n",
    "    \n",
    "def relu(a):\n",
    "    return np.maximum(0, a)\n",
    "\n",
    "def tanh(a):\n",
    "    return np.tanh(a)\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 7]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2], [3,4]])\n",
    "print(np.sum(a, axis=1, keepdims=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(a, W, B, activation):\n",
    "    z = W.dot(a) + B\n",
    "    next_a = activation(z)\n",
    "    return next_a, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11236666666666667\n",
      "3.319535322727979\n",
      "0.11236666666666667\n",
      "3.119375400077597\n",
      "0.11236666666666667\n",
      "2.9963338236731216\n",
      "0.11236666666666667\n",
      "2.9158882524374246\n",
      "0.11236666666666667\n",
      "2.8631297720859963\n",
      "0.11236666666666667\n",
      "2.8292817958966\n",
      "0.11236666666666667\n",
      "2.8082619589631705\n",
      "0.11236666666666667\n",
      "2.7957033103142095\n",
      "0.11236666666666667\n",
      "2.78851461583997\n",
      "0.11236666666666667\n",
      "2.784583088789321\n",
      "0.11236666666666667\n",
      "2.782531552684577\n",
      "0.11236666666666667\n",
      "2.781510356454471\n",
      "0.11236666666666667\n",
      "2.78102514778942\n",
      "0.11236666666666667\n",
      "2.780804829064821\n",
      "0.11236666666666667\n",
      "2.7807090835834\n",
      "0.11236666666666667\n",
      "2.7806691977856826\n",
      "0.11236666666666667\n",
      "2.78065324481846\n",
      "0.11236666666666667\n",
      "2.7806471092688434\n",
      "0.11236666666666667\n",
      "2.780644836872429\n",
      "0.11236666666666667\n",
      "2.780644025306147\n",
      "0.11236666666666667\n",
      "2.7806437454561723\n",
      "0.11236666666666667\n",
      "2.7806436521728997\n",
      "0.11236666666666667\n",
      "2.7806436220815334\n",
      "0.11236666666666667\n",
      "2.7806436126779857\n",
      "0.11236666666666667\n",
      "2.7806436098284255\n",
      "0.11236666666666667\n",
      "2.780643608990309\n",
      "0.11236666666666667\n",
      "2.780643608750854\n",
      "0.11236666666666667\n",
      "2.780643608684343\n",
      "0.11236666666666667\n",
      "2.7806436086663595\n",
      "0.11236666666666667\n",
      "2.780643608661632\n",
      "0.11236666666666667\n",
      "2.7806436086604167\n",
      "0.11236666666666667\n",
      "2.780643608660114\n",
      "0.11236666666666667\n",
      "2.7806436086600415\n",
      "0.11236666666666667\n",
      "2.780643608660028\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n",
      "0.11236666666666667\n",
      "2.7806436086600224\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-600b18a07e64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-161-600b18a07e64>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-156-7e29e1f7774e>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(a, W, B, activation)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mnext_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnext_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class model():\n",
    "    \n",
    "    def __init__(self, input_data):\n",
    "        self.learning_rate = 0.01\n",
    "        self.input_data = input_data\n",
    "        self.n, self.m = input_data.shape\n",
    "        self.first_layer_nodes = 128\n",
    "        self.output_layer_nodes = 10\n",
    "\n",
    "        self.W1 = np.random.random((self.first_layer_nodes, self.n))\n",
    "        self.B1 = np.random.random((self.first_layer_nodes, 1))\n",
    "        self.W2 = np.random.random((self.output_layer_nodes, self.first_layer_nodes))\n",
    "        self.B2 = np.random.random((self.output_layer_nodes, 1))\n",
    "    \n",
    "    def forward_pass(self): \n",
    "        self.x = self.input_data\n",
    "        self.a1, self.z1 = forward_pass(self.x, self.W1, self.B1, sigmoid)\n",
    "        self.a2, self.z2 = forward_pass(self.a1, self.W2, self.B2, softmax)\n",
    "        return cross_entropy_loss(self.a2, train_y)\n",
    "    \n",
    "    def backward_pass(self, epoch):\n",
    "        dz2 = (self.a2 - train_y)\n",
    "        dw2 = dz2.dot(self.a1.T) * 1./self.m\n",
    "        db2 = np.sum(dz2, axis=1, keepdims=True)* 1./self.m\n",
    "        \n",
    "        da1 = self.W2.T.dot(dz2)\n",
    "        dz1 = da1 * sigmoid(self.z1) * (1 - sigmoid(self.z1))\n",
    "        dw1 = dz1.dot(self.x.T)* 1./self.m\n",
    "        db1 = np.sum(dz1, axis=1, keepdims=True)* 1./self.m\n",
    "        self.learning_rate /= (1 + epoch * 0.1)\n",
    "        self.W1 -= self.learning_rate * dw1\n",
    "        self.B1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dw2\n",
    "        self.B2 -= self.learning_rate * db2\n",
    "        \n",
    "    def error():\n",
    "        pass\n",
    "\n",
    "m = model(train_x)\n",
    "for i in range(100):\n",
    "    loss = m.forward_pass()\n",
    "    predictions = np.argmax(m.a2, axis=0)\n",
    "    correct = np.argmax(train_y, axis=0)\n",
    "    print(np.sum(np.array(predictions == correct))/m.a2.shape[1])\n",
    "    print(loss)\n",
    "\n",
    "    m.backward_pass(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = Sequential()\n",
    "km.add(Dense(128, activation='tanh', input_shape=(28*28,)))\n",
    "km.add(Dense(10, activation='softmax'))\n",
    "km.compile(optimizer=\"SGD\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.4927 - acc: 0.0895\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.4589 - acc: 0.0936\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 2.4282 - acc: 0.1002\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.4000 - acc: 0.1077\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.3739 - acc: 0.1165\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.3496 - acc: 0.1258\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.3267 - acc: 0.1355\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.3050 - acc: 0.1463\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.2844 - acc: 0.1580\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.2646 - acc: 0.1701\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.2455 - acc: 0.1833\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.2271 - acc: 0.1974\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 2.2093 - acc: 0.2119\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1919 - acc: 0.2261\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.1749 - acc: 0.2417\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1584 - acc: 0.2557\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1422 - acc: 0.2695\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1263 - acc: 0.2831\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1107 - acc: 0.2959\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0953 - acc: 0.3074\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.0803 - acc: 0.3189\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0654 - acc: 0.3308\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0508 - acc: 0.3421\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0364 - acc: 0.3529\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0222 - acc: 0.3641\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0081 - acc: 0.3743\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9943 - acc: 0.3851\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9807 - acc: 0.3954\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9672 - acc: 0.4052\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9539 - acc: 0.4157\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9407 - acc: 0.4258\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9278 - acc: 0.4363\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9149 - acc: 0.4455\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9023 - acc: 0.4549\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8898 - acc: 0.4640\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8774 - acc: 0.4730\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8652 - acc: 0.4814\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8531 - acc: 0.4893\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8412 - acc: 0.4974\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8294 - acc: 0.5051\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.8178 - acc: 0.5129\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.8063 - acc: 0.5209\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.7949 - acc: 0.5279\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7837 - acc: 0.5353\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.7725 - acc: 0.5425\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7616 - acc: 0.5493\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7507 - acc: 0.5558\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7400 - acc: 0.5627\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7294 - acc: 0.5689\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7189 - acc: 0.5748\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7086 - acc: 0.5802\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.6984 - acc: 0.5859\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6883 - acc: 0.5916\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.6783 - acc: 0.5966\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6684 - acc: 0.6014\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6586 - acc: 0.6063\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6490 - acc: 0.6112\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6395 - acc: 0.6152\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6300 - acc: 0.6193\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6207 - acc: 0.6237\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6115 - acc: 0.6278\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6024 - acc: 0.6317\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5934 - acc: 0.6360\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5845 - acc: 0.6395\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5757 - acc: 0.6436\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5671 - acc: 0.6476\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5585 - acc: 0.6514\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5500 - acc: 0.6546\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5416 - acc: 0.6577\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5333 - acc: 0.6609\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5251 - acc: 0.6642\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5170 - acc: 0.6672\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5090 - acc: 0.6703\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5010 - acc: 0.6733\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4932 - acc: 0.6759\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4855 - acc: 0.6789\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4778 - acc: 0.6817\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4702 - acc: 0.6843\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4628 - acc: 0.6869\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4554 - acc: 0.6889\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4480 - acc: 0.6914\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4408 - acc: 0.6933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4336 - acc: 0.6952\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4266 - acc: 0.6976\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4196 - acc: 0.6996\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4126 - acc: 0.7017\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4058 - acc: 0.7037\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3990 - acc: 0.7056\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3923 - acc: 0.7075\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3857 - acc: 0.7096\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3792 - acc: 0.7116\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3727 - acc: 0.7134\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3663 - acc: 0.7153\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3600 - acc: 0.7169\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3537 - acc: 0.7186\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3475 - acc: 0.7207\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3414 - acc: 0.7221\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3353 - acc: 0.7238\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3293 - acc: 0.7254\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3234 - acc: 0.7270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e51703d9e8>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(train_x, train_y, batch_size=60000, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.array([[1,2,3],[1,4,2]])\n",
    "np.argmax(a, axis=1)\n",
    "a.max(axis=1, keepdims=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/2015xli/multilayer-perceptron/blob/master/multilayer-perceptron-batch.ipynb\n",
    "https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
