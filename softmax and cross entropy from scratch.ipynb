{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading the mnist dataset from the keras datasets api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e576736f60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the (28,28) dimensional matrix to a vector of size (28*28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))\n",
    "    x = x[:] / 255.\n",
    "    return x \n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    res = np.zeros((y.shape[0], num_classes))\n",
    "    res[np.arange(y.shape[0]), y] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = preprocess(train_x)\n",
    "train_y = to_categorical(train_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is an activation function that outputs probabilities for multi-class classification problems. It is similar to a sigmoid output which is often used for outputing a single probability. Softmax makes sure that the sum of the individual probabilities equals to 1. \n",
    "\n",
    "Softmax formula:\n",
    "\n",
    "$p_i = \\dfrac{e^{a_i}}{\\sum^N_{k=1} e^{a_k}}$ - Equation 1: Softmax\n",
    "\n",
    "By using the exponential function it gives more weight to higher probabilities. It gives an advantage to higher values. Therefore it's called a soft max function. A max function would give 100% probability to the highest value, softmax is somewhere in between max and an actual probability as given by equation 2:\n",
    "\n",
    "$p_i = \\dfrac{a_i}{\\sum^N_{k=1} a_k}$ - Equation 2: Standard linear probability\n",
    "\n",
    "There is however a problem with using the regular softmax function, as it uses an exponential function chances are high that it will encounter an overflow. To overcome this, we can subtract the a values by its maximum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_term = np.exp(a - np.max(a, axis=1, keepdims=True)) #-max to Prevent overflow\n",
    "    res = exp_term/np.sum(exp_term, axis=1, keepdims=True)\n",
    "    return res\n",
    "\n",
    "def cross_entropy_loss(outputs, y):\n",
    "    o = np.clip(outputs, 1e-7, 1 - 1e-7) #Prevent divide by zero warnings\n",
    "    loss = -np.sum(y * np.log(o), axis=1)\n",
    "    return np.sum(loss) / outputs.shape[0] #average loss of all samples\n",
    "    \n",
    "def relu(a):\n",
    "    return np.maximum(0, a)\n",
    "\n",
    "def tanh(a):\n",
    "    return np.tanh(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 7]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2], [3,4]])\n",
    "print(np.sum(a, axis=1, keepdims=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(a, W, B, activation):\n",
    "    z = a.dot(W) + B\n",
    "    next_a = activation(z)\n",
    "    return next_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.77626365e-02 7.37209620e-03 1.71534329e-05 9.81799914e-05\n",
      " 1.28840546e-02 7.45288127e-03 8.34759579e-01 7.87953848e-02\n",
      " 1.85850079e-02 2.22730267e-02]\n",
      "4.897618764271143\n",
      "[0.00000000e+000 1.00000000e+000 2.34762621e-195 2.11905312e-100\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000]\n",
      "14.3069589808823\n",
      "[0.0000000e+000 0.0000000e+000 1.3273565e-192 1.0000000e+000\n",
      " 0.0000000e+000 0.0000000e+000 0.0000000e+000 0.0000000e+000\n",
      " 0.0000000e+000 0.0000000e+000]\n",
      "14.471094920576226\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "14.517568762748157\n",
      "[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 2.15471673e-018 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 1.06427321e-189 5.61293545e-094]\n",
      "14.52697098515288\n",
      "[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 5.44279379e-167 2.29862011e-016]\n",
      "14.548730414146673\n",
      "[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 2.95696563e-206 1.00000000e+000]\n",
      "14.5199864770808\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "14.546312699814033\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "14.435097840512418\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "14.661825717929233\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "14.528314159782125\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "14.3069589808823\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "14.471094920576226\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "14.517568762748157\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "14.435097840512418\n",
      "[1.05245564e-67 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00]\n",
      "14.5199864770808\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-292e31b52a00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-292e31b52a00>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtanh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-120-419af8e8080b>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(a, W, B, activation)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mnext_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnext_a\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class model():\n",
    "    def __init__(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        self.m, self.n = input_data.shape\n",
    "        self.first_layer_nodes = 128\n",
    "        self.output_layer_nodes = 10\n",
    "\n",
    "        self.W1 = np.random.random((self.n, self.first_layer_nodes))\n",
    "        self.B1 = np.random.random((1, self.first_layer_nodes))\n",
    "        self.W2 = np.random.random((self.first_layer_nodes, self.output_layer_nodes))\n",
    "        self.B2 = np.random.random((1, self.output_layer_nodes))\n",
    "    \n",
    "    def forward_pass(self): \n",
    "        self.a1 = self.input_data\n",
    "        self.a2 = forward_pass(self.a1, self.W1, self.B1, tanh)\n",
    "        self.output = forward_pass(self.a2, self.W2, self.B2, softmax)\n",
    "        return cross_entropy_loss(self.output, train_y)\n",
    "    \n",
    "    def backward_pass(self):\n",
    "        dw2 = self.a2.T.dot(self.output - train_y)\n",
    "        db2 = np.sum(self.output - train_y, axis=0, keepdims=True)\n",
    "        \n",
    "        da2 = (self.output - train_y).dot((self.W2).T)\n",
    "        dz1 = da2 * (1 - np.power(self.a2, 2))\n",
    "        dw1 = self.a1.T.dot(dz1)\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        self.W1 -= 0.01 * dw1\n",
    "        self.B1 -= 0.01 * db1\n",
    "        self.W2 -= 0.01 * dw2\n",
    "        self.B2 -= 0.01 * db2\n",
    "        \n",
    "        \n",
    "    def error():\n",
    "        pass\n",
    "\n",
    "m = model(train_x)\n",
    "for i in range(100):\n",
    "    loss = m.forward_pass()\n",
    "    print(loss)\n",
    "    m.backward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = Sequential()\n",
    "km.add(Dense(128, activation='tanh', input_shape=(28*28,)))\n",
    "km.add(Dense(10, activation='softmax'))\n",
    "km.compile(optimizer=\"SGD\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.4927 - acc: 0.0895\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.4589 - acc: 0.0936\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 2.4282 - acc: 0.1002\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.4000 - acc: 0.1077\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.3739 - acc: 0.1165\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.3496 - acc: 0.1258\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.3267 - acc: 0.1355\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.3050 - acc: 0.1463\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.2844 - acc: 0.1580\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.2646 - acc: 0.1701\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.2455 - acc: 0.1833\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.2271 - acc: 0.1974\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 2.2093 - acc: 0.2119\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1919 - acc: 0.2261\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.1749 - acc: 0.2417\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1584 - acc: 0.2557\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1422 - acc: 0.2695\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1263 - acc: 0.2831\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.1107 - acc: 0.2959\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0953 - acc: 0.3074\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 2.0803 - acc: 0.3189\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0654 - acc: 0.3308\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0508 - acc: 0.3421\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0364 - acc: 0.3529\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0222 - acc: 0.3641\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 2.0081 - acc: 0.3743\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9943 - acc: 0.3851\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9807 - acc: 0.3954\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9672 - acc: 0.4052\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9539 - acc: 0.4157\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9407 - acc: 0.4258\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9278 - acc: 0.4363\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9149 - acc: 0.4455\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.9023 - acc: 0.4549\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8898 - acc: 0.4640\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8774 - acc: 0.4730\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8652 - acc: 0.4814\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8531 - acc: 0.4893\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8412 - acc: 0.4974\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.8294 - acc: 0.5051\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.8178 - acc: 0.5129\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.8063 - acc: 0.5209\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.7949 - acc: 0.5279\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7837 - acc: 0.5353\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.7725 - acc: 0.5425\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7616 - acc: 0.5493\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7507 - acc: 0.5558\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7400 - acc: 0.5627\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7294 - acc: 0.5689\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7189 - acc: 0.5748\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.7086 - acc: 0.5802\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.6984 - acc: 0.5859\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6883 - acc: 0.5916\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.6783 - acc: 0.5966\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6684 - acc: 0.6014\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6586 - acc: 0.6063\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6490 - acc: 0.6112\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6395 - acc: 0.6152\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6300 - acc: 0.6193\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6207 - acc: 0.6237\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6115 - acc: 0.6278\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.6024 - acc: 0.6317\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5934 - acc: 0.6360\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5845 - acc: 0.6395\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5757 - acc: 0.6436\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5671 - acc: 0.6476\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5585 - acc: 0.6514\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5500 - acc: 0.6546\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5416 - acc: 0.6577\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5333 - acc: 0.6609\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5251 - acc: 0.6642\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5170 - acc: 0.6672\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.5090 - acc: 0.6703\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.5010 - acc: 0.6733\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4932 - acc: 0.6759\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4855 - acc: 0.6789\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4778 - acc: 0.6817\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4702 - acc: 0.6843\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4628 - acc: 0.6869\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.4554 - acc: 0.6889\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4480 - acc: 0.6914\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4408 - acc: 0.6933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4336 - acc: 0.6952\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4266 - acc: 0.6976\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4196 - acc: 0.6996\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4126 - acc: 0.7017\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.4058 - acc: 0.7037\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3990 - acc: 0.7056\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3923 - acc: 0.7075\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3857 - acc: 0.7096\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3792 - acc: 0.7116\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3727 - acc: 0.7134\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3663 - acc: 0.7153\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3600 - acc: 0.7169\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3537 - acc: 0.7186\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3475 - acc: 0.7207\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3414 - acc: 0.7221\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3353 - acc: 0.7238\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3293 - acc: 0.7254\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 1.3234 - acc: 0.7270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e51703d9e8>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(train_x, train_y, batch_size=60000, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
